{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import keras\n",
    "import random\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from keras.applications.mobilenetv2 import MobileNetV2\n",
    "from keras.preprocessing import image\n",
    "from keras.engine import Layer\n",
    "from keras.applications.inception_resnet_v2 import preprocess_input\n",
    "from keras.layers import Conv2D, UpSampling2D, InputLayer, Conv2DTranspose, Input, Reshape, merge, concatenate, Activation, Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import TensorBoard \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import RepeatVector, Permute\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from skimage.color import rgb2lab, lab2rgb, rgb2gray, gray2rgb\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imsave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image transformer\n",
    "image_gen = ImageDataGenerator(\n",
    "        shear_range=0.4,\n",
    "        zoom_range=0.4,\n",
    "        rotation_range=40,\n",
    "        horizontal_flip=True)\n",
    "#Generate training data\n",
    "batch_size = 20\n",
    "\n",
    "# function to split training set X train, y train and produce augmented images\n",
    "train_dir = '../Capstone/few_images/'\n",
    "def image_a_b_gen(batch_size):\n",
    "    for i in image_gen.flow_from_directory(train_dir, batch_size=batch_size,class_mode=None,shuffle=True):\n",
    "        i = 1.0/255*i\n",
    "        grayscaled_rgb = gray2rgb(rgb2gray(i))\n",
    "        embed = mobile_input(grayscaled_rgb)\n",
    "        lab_batch = rgb2lab(i)\n",
    "        X_train = lab_batch[:,:,:,0]\n",
    "        X_train = X_train.reshape(X_train.shape+(1,))\n",
    "        y_train = lab_batch[:,:,:,1:]/128\n",
    "        yield ([X_train, second_input(grayscaled_rgb)], y_train)\n",
    "\n",
    "        \n",
    "#Create embedding\n",
    "def second_input(grayscaled_rgb):\n",
    "    grayscaled_rgb_resized = []\n",
    "    for i in grayscaled_rgb:\n",
    "        i = resize(i, (224, 224, 3), mode='constant')\n",
    "        grayscaled_rgb_resized.append(i)\n",
    "    grayscaled_rgb_resized = np.array(grayscaled_rgb_resized)\n",
    "    return grayscaled_rgb_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaled model input\n",
    "scaled_input = Input(shape=(224, 224, 1))\n",
    "global_output = Conv2D(64, (3,3), activation='relu', padding='same', strides=2)(scaled_input)\n",
    "global_output = Conv2D(128, (3,3), activation='relu', padding='same')(global_output)\n",
    "global_output = Conv2D(128, (3,3), activation='relu', padding='same', strides=2)(global_output)\n",
    "global_output = Conv2D(256, (3,3), activation='relu', padding='same')(global_output)\n",
    "global_output = Conv2D(256, (3,3), activation='relu', padding='same', strides=2)(global_output)\n",
    "global_output = Conv2D(512, (3,3), activation='relu', padding='same')(global_output)\n",
    "global_output = Conv2D(512, (3,3), activation='relu', padding='same',strides=2)(global_output)\n",
    "global_output = Conv2D(512, (3,3), activation='relu', padding='same')(global_output)\n",
    "global_output = Conv2D(512, (3,3), activation='relu', padding='same',strides=2)(global_output)\n",
    "global_output = Conv2D(512, (3,3), activation='relu', padding='same')(global_output)\n",
    "global_output = Flatten()(global_output)\n",
    "global_output = Dense(1024, activation='relu')(global_output)\n",
    "global_output = Dense(512, activation='relu')(global_output)\n",
    "global_output = Dense(256, activation='relu')(global_output)\n",
    "#Encoder\n",
    "encoder_input = Input(shape=(256, 256, 1))\n",
    "encoder_output = Conv2D(64, (3,3), activation='relu', padding='same', strides=2)(encoder_input)\n",
    "encoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(128, (3,3), activation='relu', padding='same', strides=2)(encoder_output)\n",
    "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same', strides=2)(encoder_output)\n",
    "encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "#Fusion\n",
    "fusion_output = RepeatVector(32 * 32)(global_output) \n",
    "fusion_output = Reshape(([32, 32, 256]))(fusion_output)\n",
    "fusion_output = concatenate([encoder_output, fusion_output], axis=3) \n",
    "fusion_output = Conv2D(256, (1, 1), activation='relu', padding='same')(fusion_output)\n",
    "#Decoder\n",
    "decoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(fusion_output)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "decoder_output = Conv2D(64, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "decoder_output = Conv2D(32, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "decoder_output = Conv2D(16, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "decoder_output = Conv2D(2, (3, 3), activation='tanh', padding='same')(decoder_output)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "model = Model(inputs=[encoder_input, scaled_input], outputs=decoder_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit_generator(image_a_b_gen(batch_size), epochs=10, steps_per_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('C:/Users/n3rDx/Desktop/Homework Upload/Capstone/4th_expt3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load black and white images\n",
    "gray_import = []\n",
    "for filename in os.listdir('../Capstone/test_images/test'):\n",
    "    gray_import.append(img_to_array(load_img('C:/Users/n3rDx/Desktop/Homework Upload/Capstone/test_images/test/'+filename)))\n",
    "gray_import = 1.0/255*(np.array(gray_import, dtype=float))\n",
    "inception_embedding = inception_input(gray_import)\n",
    "gray_import = rgb2lab(gray2rgb(rgb2gray(gray_import)))[:,:,:,0]\n",
    "gray_import = gray_import.reshape(gray_import.shape+(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gray import, get the AB predictions\n",
    "output = model.predict([gray_import,inception_embedding])\n",
    "output = output*128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output colorizations\n",
    "for i in range(len(output)):\n",
    "    canvas = np.zeros((256, 256, 3))\n",
    "    canvas[:,:,0] = gray_import[i][:,:,0]\n",
    "    canvas[:,:,1:] = output[i]\n",
    "    picture = lab2rgb(canvas)*255\n",
    "    imsave(\"C:/Users/n3rDx/Desktop/Homework Upload/Capstone/results/\"+str(i)+\".jpg\", picture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory error, so to feed model in batches\n",
    "# Image transformer\n",
    "batch_size = 20\n",
    "\n",
    "image_gen = ImageDataGenerator(\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        rotation_range=20,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# function to split training set X train, y train and produce augmented images\n",
    "train_dir = '../Capstone/few_images/'\n",
    "def image_a_b_gen(batch_size):\n",
    "    for i in image_gen.flow_from_directory(train_dir, batch_size=batch_size,class_mode=None,shuffle=True):\n",
    "        i = 1.0/255*i\n",
    "        lab_batch = rgb2lab(i)\n",
    "        X_train = lab_batch[:,:,:,0]\n",
    "        X_train = X_train.reshape(X_train.shape+(1,))\n",
    "        y_train = lab_batch[:,:,:,1:]/128\n",
    "        yield ([X_train, y_train])\n",
    "\n",
    "test_gen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shared models\n",
    "\n",
    "\n",
    "#Model A\n",
    "encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output_shared)\n",
    "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "#Model B\n",
    "\n",
    "#Fusion \n",
    "fusion_output = concatenate([encoder_output, global_encoder], axis=3) \n",
    "fusion_output = Conv2D(256, (1, 1), activation='relu', padding='same')(fusion_output)\n",
    "#Decoder\n",
    "decoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(fusion_output)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "decoder_output = Conv2D(64, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "decoder_output = Conv2D(32, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "decoder_output = Conv2D(16, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "decoder_output = Conv2D(2, (3, 3), activation='tanh', padding='same')(decoder_output)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "\n",
    "model = Model(inputs=encoder_input, outputs=decoder_output)\n",
    "# Finish model\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "model.fit_generator(image_a_b_gen(batch_size=batch_size), steps_per_epoch=10, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
